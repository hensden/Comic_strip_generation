<h1>UNet: semantic segmentation with PyTorch</h1>

<p><img src="https://framapic.org/OcE8HlU6me61/KNTt8GFQzxDR.png" alt="input and output for a random image in the test dataset" title="" /></p>

<p>Customized implementation of the <a href="https://arxiv.org/abs/1505.04597">U-Net</a> in PyTorch for Kaggle's <a href="https://www.kaggle.com/c/carvana-image-masking-challenge">Carvana Image Masking Challenge</a> from high definition images.</p>

<p>This model was trained from scratch with 5000 images (no data augmentation) and scored a <a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">dice coefficient</a> of 0.988423 (511 out of 735) on over 100k test images. This score could be improved with more training, data augmentation, fine tuning, playing with CRF post-processing, and applying more weights on the edges of the masks.</p>

<p>The Carvana data is available on the <a href="https://www.kaggle.com/c/carvana-image-masking-challenge/data">Kaggle website</a>.</p>

<h2>Usage</h2>

<p><strong>Note : Use Python 3</strong></p>

<h3>Prediction</h3>

<p>You can easily test the output masks on your images via the CLI.</p>

<p>To predict a single image and save it:</p>

<p><code>python predict.py -i image.jpg -o output.jpg</code></p>

<p>To predict a multiple images and show them without saving them:</p>

<p><code>python predict.py -i image1.jpg image2.jpg --viz --no-save</code></p>

<p>```shell script</p>

<blockquote>
  <p>python predict.py -h
usage: predict.py [-h] [--model FILE] --input INPUT [INPUT ...]
                  [--output INPUT [INPUT ...]] [--viz] [--no-save]
                  [--mask-threshold MASK_THRESHOLD] [--scale SCALE]</p>
</blockquote>

<p>Predict masks from input images</p>

<p>optional arguments:
  -h, --help            show this help message and exit
  --model FILE, -m FILE
                        Specify the file in which the model is stored
                        (default: MODEL.pth)
  --input INPUT [INPUT ...], -i INPUT [INPUT ...]
                        filenames of input images (default: None)
  --output INPUT [INPUT ...], -o INPUT [INPUT ...]
                        Filenames of ouput images (default: None)
  --viz, -v             Visualize the images as they are processed (default:
                        False)
  --no-save, -n         Do not save the output masks (default: False)
  --mask-threshold MASK<em>THRESHOLD, -t MASK</em>THRESHOLD
                        Minimum probability value to consider a mask pixel
                        white (default: 0.5)
  --scale SCALE, -s SCALE
                        Scale factor for the input images (default: 0.5)
<code>``
You can specify which model file to use with</code>--model MODEL.pth`.</p>

<h3>Training</h3>

<p>```shell script</p>

<blockquote>
  <p>python train.py -h
usage: train.py [-h] [-e E] [-b [B]] [-l [LR]] [-f LOAD] [-s SCALE] [-v VAL]</p>
</blockquote>

<p>Train the UNet on images and target masks</p>

<p>optional arguments:
  -h, --help            show this help message and exit
  -e E, --epochs E      Number of epochs (default: 5)
  -b [B], --batch-size [B]
                        Batch size (default: 1)
  -l [LR], --learning-rate [LR]
                        Learning rate (default: 0.1)
  -f LOAD, --load LOAD  Load model from a .pth file (default: False)
  -s SCALE, --scale SCALE
                        Downscaling factor of the images (default: 0.5)
  -v VAL, --validation VAL
                        Percent of the data that is used as validation (0-100)
                        (default: 15.0)</p>

<p><code>``
By default, the</code>scale` is 0.5, so if you wish to obtain better results (but use more memory), set it to 1.</p>

<p>The input images and target masks should be in the <code>data/imgs</code> and <code>data/masks</code> folders respectively.</p>

<h2>Dependencies</h2>

<p>This package depends on <a href="https://github.com/lucasb-eyer/pydensecrf">pydensecrf</a>, available via <code>pip install</code>.</p>

<h2>Notes on memory</h2>

<p>The model has be trained from scratch on a GTX970M 3GB.
Predicting images of 1918*1280 takes 1.5GB of memory.
Training takes much approximately 3GB, so if you are a few MB shy of memory, consider turning off all graphical displays.
This assumes you use bilinear up-sampling, and not transposed convolution in the model.</p>

<hr />

<p>Original paper by Olaf Ronneberger, Philipp Fischer, Thomas Brox: <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>

<p><img src="https://i.imgur.com/jeDVpqF.png" alt="network architecture" title="" /></p>
